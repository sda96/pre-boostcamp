{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "conditional-smooth",
   "metadata": {},
   "source": [
    "### 신경망 이해하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-minimum",
   "metadata": {},
   "source": [
    "- 기존에 데이터를 선형모델로 해석하였지만 신경망은 비선형모델에 해당합니다\n",
    "- 신경망을 수학적 수식으로 분해하면 선형모델이 존재하며 신경망은 선형모델과 비선형모델의 구성으로 이루어져 있습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-valley",
   "metadata": {},
   "source": [
    "#### 신경망 수식의 선형모델인 부분"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-contract",
   "metadata": {},
   "source": [
    "<img src=\"https://imgur.com/1Bmk846.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-zimbabwe",
   "metadata": {},
   "source": [
    "#### 소프트맥스"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-theme",
   "metadata": {},
   "source": [
    "- 소프트맥스 함수는 모델의 출력을 확률로 해석할 수 있게 변환해주는 함수 입니다\n",
    "- 분류문제를 해결할 때 선형모델과 소프트맥스 함수를 결합하여 예측합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-calibration",
   "metadata": {},
   "source": [
    "<img src=\"https://imgur.com/RgcL6ev.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dominant-contamination",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(vec):\n",
    "    # 분자가 지수함수이기 떄문에 오버플로우 문제를 예방하기 위하여 max 값을 빼줍니다\n",
    "    denumerator = np.exp(vec - np.max(vec, axis=-1, keepdims=True)) \n",
    "    numerator = np.sum(denumerator, axis=-1, keepdims=True)\n",
    "    val = denumerator / numerator\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "designed-setup",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.44728471e-01, 6.65240956e-01, 9.00305732e-02],\n",
       "       [9.00305732e-02, 2.44728471e-01, 6.65240956e-01],\n",
       "       [2.06106005e-09, 4.53978686e-05, 9.99954600e-01]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = np.array([[1,2,0],[-1,0,1],[-10,0,10]])\n",
    "softmax(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-attachment",
   "metadata": {},
   "source": [
    "단, 학습이 아니라 추론을 할 때에는, 원핫벡터로 최대값을 가진 인덱스만 1로 출력하는 연산을 사용하므로 softmax 함수를 굳이 사용할 필요가 없습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-hamburg",
   "metadata": {},
   "source": [
    "#### 신경망 수식의 비선형모델이 포함된 부분"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-spanish",
   "metadata": {},
   "source": [
    "- 신경망은 선형모델로 나온 출력값에 활성화함수를 합성한 함수 입니다\n",
    "- 활성화함수는 선형모델이 아닌 비선형 함수로 선형모델을 비선형모델로 변환시켜 줍니다"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "acute-present",
   "metadata": {},
   "source": [
    "<img src=\"https://imgur.com/8584Fip.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-trading",
   "metadata": {},
   "source": [
    "#### 활성함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-preserve",
   "metadata": {},
   "source": [
    "- 활성함수는 실수공간 위에 정의된 비선형 함수로써 딥러닝에서 매우 중요한 개념이니다\n",
    "- 활성홤수를 쓰지 않으면 딥러닝은 선형모형과 차이가 없어집니다\n",
    "- 시그모이드 함수나 탄젠트 함수는 전통적으로 많이 쓰이던 활성함수지만 딥러닝에선 relu 함수를 많이 쓰고 있습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "armed-cassette",
   "metadata": {},
   "source": [
    "<img src=\"https://imgur.com/p0pPB79.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "presidential-hammer",
   "metadata": {},
   "source": [
    "신경망은 활성함수와 선형모델의 결합으로 표현된 함수입니다  \n",
    "다수의 입력값을 받아서 하나의 출력값으로 반환해주는 알고리즘을 퍼셉트론이라고 부르며  \n",
    "이러한 퍼셉트론이 여러층으로 합성된 함수르 다층 퍼셉트론(MLP)라고 부릅니다  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-custom",
   "metadata": {},
   "source": [
    "#### 순전파"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-headquarters",
   "metadata": {},
   "source": [
    "- 순전파는 학습이 아닌 주어진 입력이 들어오면 출력을 내뱉는 연산입니다"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "going-sigma",
   "metadata": {},
   "source": [
    "<img src=\"https://imgur.com/sV3663s.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-cache",
   "metadata": {},
   "source": [
    "#### 층을 쌓는 이유"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-renaissance",
   "metadata": {},
   "source": [
    "- 층을 쌓는 이유는 적은 뉴런, 노드로 좀 더 복잡한 목적함수를 근사할 수 있기 때문입니다\n",
    "- 이론적으로는 2층 신경망으로도 임의의 연속함수를 근사할 수 있습니다, 이를 universal approximation theorem 이라고 부릅니다\n",
    "- 그러나 층이 깊을수록 목적함수를 근사하는데 필요한 뉴런(노드)의 숫자가 훨씬 빨리 줄어들어 좀 더 효율적으로 학습이 가능합니다\n",
    "- 층이 얕으면 뉴런의 숫자가 기하급수적으로 늘어나서 넓은(wide) 신경망이 되어야 합니다\n",
    "- 층이 깊을수록 최적화가 어려워지기에 학습이 더 어려워 질 수 있습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-courage",
   "metadata": {},
   "source": [
    "#### 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-extreme",
   "metadata": {},
   "source": [
    "- 딥러닝은 역전파 알고리즘을 이용하여 각 층에서 사용된 파라미터를 학습합니다\n",
    "- 파라미터를 학습하기 위해선 각 층의 그래디언트 벡터를 계산해야 합니다\n",
    "- 이전층의 그래디언트 벡터를 사용하여 다음층의 그래디언트 벡터를 역순으로 계산하는데 이때 연쇄법칙을 사용합니다\n",
    "- 역전파 알고리즘은 합성함수 미분법인 연쇄법칙 기반 자동미분을 사용합나디"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-malawi",
   "metadata": {},
   "source": [
    "<img src=\"https://imgur.com/BEXoHfI.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-velvet",
   "metadata": {},
   "source": [
    "신경망은 여러 층이 겹쳐진 합성함수입니다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
