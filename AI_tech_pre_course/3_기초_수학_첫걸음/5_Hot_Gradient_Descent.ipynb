{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fluid-feature",
   "metadata": {},
   "source": [
    "### 선형회귀분석 복습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-vietnam",
   "metadata": {},
   "source": [
    "무어 펜로즈 역행렬을 사용하지 않고 경사하강법을 이용한 최적회 회귀선을 구하겠습니다\n",
    "- 사용 이유 1 : 기존의 선형모델이 아닌 비선형모델에서도 효과를 볼 수 있습니다\n",
    "- 사용 이유 2 : 일반적인 기계학습 관점에서 사용되는 방법론을 이해하기 위함 입니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-three",
   "metadata": {},
   "source": [
    "<img src=\"https://imgur.com/wD7uTd3.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-amendment",
   "metadata": {},
   "source": [
    "#### 경사하강법 사용시 고려사항"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-transfer",
   "metadata": {},
   "source": [
    "- 경사하강법을 이용하여 최적의 회귀계수를 구하는 데, 학습률을 너무 작게 주는 경우 최적화가 제대로 이루어 지지 않을 수도 있습니다  \n",
    "- 이론적으로 경사하강법은 미분가능하고 볼록한 함수에 대해선 하이퍼파라미터인 적절한 학습률과 학습횟수를 정해야 수렴이 보장됩니다\n",
    "- 하지만 비선형 호귀 문제의 경우 목적식이 볼록하지 않을 수 있으므로 수렴이 항상 보장되지는 않습니다.\n",
    "- 볼록하지 않은(non-convex) 함수의 경우 볼록한 부분이 2개 이상이거나, 아예 볼록한 부분을 찾을 수 없는 경우가 있습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-ghana",
   "metadata": {},
   "source": [
    "### 확률적 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-election",
   "metadata": {},
   "source": [
    "- 확률적 경사하강법(stochastic gradient descent)는 모든 데이터를 사용해서 업데이트하는 대신 데이터 한 개 또는 일부 활용하여 업데이트 합니다\n",
    "- 볼록이 아닌 목적식은 SGD를 통해 최적화할 수 있습니다\n",
    "- 여러개의 미니 배치 SGD의 성능이 기존의 경사하강법과 유사할 것이다라는 것이 확률적으로 보장됩니다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-retention",
   "metadata": {},
   "source": [
    "<img src=\"https://imgur.com/m7hTuE3.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-bangladesh",
   "metadata": {},
   "source": [
    "미니배치는 확률적으로 선택하므로 목적식 모양이 바뀌게 됩니다 하지만 방향은 얼추 비슷한 방향으로 향하게 될 것 입니다  \n",
    "\n",
    "경사하강법에서는 극소점을 만나서 업데이트를 멈추게 되지만 SGD는 미니배치 단위로 업데이트를 하기 떄문에 확률적으로 극소점을 만나도 넘어갈 수 있게(탈출) 됩니다  \n",
    "\n",
    "SGD는 볼록이 아닌 목적식에서도 사용 가능하므로 경사하강법보다 머신러닝 학습에 더 빠르고, 효율적입니다\n",
    "\n",
    "SGD는 하드웨어 사용에 있어서 미니배치는 병렬처리도 가능하기에 메모리 효율성도 좋습니다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
